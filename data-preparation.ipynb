{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset\n",
    "=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load needed packages\n",
    "import pandas as pd\n",
    "import random \n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directory for data \n",
    "DATA_PATH = 'data/'\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "RAW_DATA = f'{DATA_PATH}raw_data/'\n",
    "os.makedirs(RAW_DATA, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Download Dataset\n",
    "First we download and extract the gigaword dataset (~3M) [here](https://drive.google.com/file/d/0B6N7tANPyVeBNmlSX19Ld2xDU1E/view?usp=sharing)\n",
    "\n",
    "Meantime using the BBC dataset (7M) here: https://www.kaggle.com/pariza/bbc-news-summary/download\n",
    "\n",
    "Data downloaded and stored under `data/BBC News Summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download data (CNN)\n",
    "# !curl --header \"Host: doc-0c-3o-docs.googleusercontent.com\" --header \"User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36\" --header \"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\" --header \"Accept-Language: en-US,en;q=0.9\" --header \"Cookie: AUTH_s4a6oitvorbtivfcjm2pefm907l1ntir=08690680304265769485|1531648800000|pm1jthdhng09cikkb0pkdcqlod4d76p8\" --header \"Connection: keep-alive\" \"https://doc-0c-3o-docs.googleusercontent.com/docs/securesc/d1n9duui70mcvt9ph3953bv4foh1d3fm/pb1h3k6beg14nfv16poorm9mr6bl90e3/1531656000000/03129501499031348422/08690680304265769485/0B6N7tANPyVeBNmlSX19Ld2xDU1E?e=download\" -o \"summary.tar.gz\" -L\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract files \n",
    "# !tar -xzf summary.tar.gz -C {RAW_DATA} && mv summary.tar.gz {RAW_DATA}\n",
    "# !gunzip {RAW_DATA}sumdata/train/*.*.txt.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass()\n",
    "class BBCNewsDataReader:\n",
    "    base_folder: str\n",
    "    exclusion: list = field(default_factory=list)\n",
    "    \n",
    "    @property\n",
    "    def news_articles_folder(self):\n",
    "        return self.base_folder + '/News Articles'\n",
    "    \n",
    "    @property\n",
    "    def summaries_folder(self):\n",
    "        return self.base_folder + '/Summaries'\n",
    "    \n",
    "    @property\n",
    "    def categories(self):\n",
    "        exclusion_folders = lambda x: x not in [\".DS_Store\"] + self.exclusion\n",
    "        return filter(exclusion_folders, os.listdir(self.news_articles_folder))\n",
    "    \n",
    "    def to_df(self):\n",
    "        df = pd.DataFrame(columns=['article', 'summary', 'category', 'filename'])\n",
    "        for article_folder, summary_folder in self.__category_folders():\n",
    "            category = article_folder.split('/')[-1]\n",
    "            for filename in os.listdir(article_folder):\n",
    "                if os.path.isfile(f'{article_folder}/{filename}'):\n",
    "                    try:\n",
    "                        article = self.__read_file(f'{article_folder}/{filename}')\n",
    "                        summary = self.__read_file(f'{summary_folder}/{filename}')\n",
    "                        df = df.append({'article': article, 'summary': summary, 'category': category, 'filename': filename}, ignore_index=True)\n",
    "                    except UnicodeDecodeError:\n",
    "                        pass\n",
    "        return df\n",
    "                \n",
    "            \n",
    "    def __category_folders(self):\n",
    "        return [\n",
    "            (f'{self.news_articles_folder}/{category}', f'{self.summaries_folder}/{category}') for category in self.categories\n",
    "        ]\n",
    "    \n",
    "    \n",
    "    def __read_file(self, filepath):\n",
    "        with open(filepath) as file:\n",
    "            return file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = BBCNewsDataReader(\n",
    "        base_folder='data/raw_data/BBC News Summary',\n",
    "        # exclusion=['entertainment', 'tech', 'sport', 'politics'] # remove these to read all data\n",
    "    ).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2224, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Musicians to tackle US red tape\\n\\nMusicians' ...</td>\n",
       "      <td>Nigel McCune from the Musicians' Union said Br...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>289.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U2's desire to be number one\\n\\nU2, who have w...</td>\n",
       "      <td>But they still want more.They have to want to ...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>262.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rocker Doherty in on-stage fight\\n\\nRock singe...</td>\n",
       "      <td>Babyshambles, which he formed after his acrimo...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>276.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Snicket tops US box office chart\\n\\nThe film a...</td>\n",
       "      <td>A Series of Unfortunate Events also stars Scot...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>060.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ocean's Twelve raids box office\\n\\nOcean's Twe...</td>\n",
       "      <td>Ocean's Twelve, the crime caper sequel starrin...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>074.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  Musicians to tackle US red tape\\n\\nMusicians' ...   \n",
       "1  U2's desire to be number one\\n\\nU2, who have w...   \n",
       "2  Rocker Doherty in on-stage fight\\n\\nRock singe...   \n",
       "3  Snicket tops US box office chart\\n\\nThe film a...   \n",
       "4  Ocean's Twelve raids box office\\n\\nOcean's Twe...   \n",
       "\n",
       "                                             summary       category filename  \n",
       "0  Nigel McCune from the Musicians' Union said Br...  entertainment  289.txt  \n",
       "1  But they still want more.They have to want to ...  entertainment  262.txt  \n",
       "2  Babyshambles, which he formed after his acrimo...  entertainment  276.txt  \n",
       "3  A Series of Unfortunate Events also stars Scot...  entertainment  060.txt  \n",
       "4  Ocean's Twelve, the crime caper sequel starrin...  entertainment  074.txt  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add Validation Split (later, using train and test for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business         510\n",
       "sport            510\n",
       "politics         417\n",
       "tech             401\n",
       "entertainment    386\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80, 20 split\n",
    "split = StratifiedShuffleSplit( n_splits = 1 , test_size = 0.2 , random_state = 42 ) \n",
    "for train_index , test_index in split.split(data, data['category']):\n",
    "    strat_train_set = data.loc[train_index]\n",
    "    strat_test_set = data.loc[test_index] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business         102\n",
       "sport            102\n",
       "politics          84\n",
       "tech              80\n",
       "entertainment     77\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_test_set.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business         408\n",
       "sport            408\n",
       "politics         333\n",
       "tech             321\n",
       "entertainment    309\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add stratified sample by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     r = random.randint(0,50)\n",
    "#     display(data.iloc[r]['article'])\n",
    "#     display(data.iloc[r]['summary'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_DATA_PATH = 'data/sample_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save sample train, val, and test datasets \n",
    "strat_train_set.to_csv(f'{SAMPLE_DATA_PATH}/train_ds.csv', index=None)\n",
    "strat_test_set.to_csv(f'{SAMPLE_DATA_PATH}/valid_ds.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Smaller Sample Data (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_train_ = strat_train_set.sample(64)\n",
    "# sample_val_ = strat_test_set.sample(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(sample_train_), len(sample_val_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save sample train, val, and test datasets \n",
    "# sample_train_.to_csv(f'{SAMPLE_DATA_PATH}train_ds_.csv', index=None)\n",
    "# sample_val_.to_csv(f'{SAMPLE_DATA_PATH}valid_ds_.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
